{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95eb4d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from numpy import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a1417-c20f-4b1d-b122-d2080adfca35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e3a9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackProcess:\n",
    "    def __init__(self, S0, r, sigma, n):\n",
    "        self.S0 = S0\n",
    "        self.r = r\n",
    "        self.sigma = sigma\n",
    "        self.n = n\n",
    "\n",
    "    def generate(self):\n",
    "        S0, r, sigma, n = self.S0, self.r, self.sigma, self.n\n",
    "        dt = 1 / 365\n",
    "        dW = np.random.normal(0, dt ** 0.5, n)\n",
    "        chg = np.ones(n + 1)\n",
    "        chg[1:] += r * dt + sigma * dW\n",
    "        accum_chg = chg.cumprod()\n",
    "        return S0 * accum_chg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acdcb22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaEnv():\n",
    "    n_observation = 5\n",
    "\n",
    "    def __init__(self, process: BlackProcess, tenor, strike):\n",
    "        self.process = process\n",
    "        self.tenor = tenor\n",
    "        self.strike = strike\n",
    "        self.t = 0\n",
    "        self.path = None\n",
    "        self.observations = None\n",
    "        self.reset()\n",
    "\n",
    "    def df(self):\n",
    "        return exp(-self.process.r / 365)\n",
    "\n",
    "    def mu(self):\n",
    "        return exp(self.process.r / 365) - 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.path = self.process.generate()\n",
    "        self.t = 0\n",
    "        self.observations = np.stack([self.observation(t) for t in range(self.tenor + 1)], 0)\n",
    "        return self.observations[0]\n",
    "\n",
    "    def St(self, t=None) -> np.float32:\n",
    "        t = self.t if t is None else t\n",
    "        return self.path[t]\n",
    "\n",
    "    def observation(self, t=None):\n",
    "        S_K = self.St(t) / self.strike\n",
    "        moneyness = max(0, S_K)\n",
    "\n",
    "        t = self.t if t is None else t\n",
    "        tenor = (self.tenor - t) / 365\n",
    "\n",
    "        obs = np.array([moneyness, moneyness ** 2, tenor, tenor ** 2, moneyness * tenor])\n",
    "        assert len(obs) == self.n_observation\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: hedge ratio, i.e. delta\n",
    "        :return: S_t0, S_t1, reward, terminated, can_early_exercise, payoff, dS\n",
    "        \"\"\"\n",
    "        S_t0 = self.observations[self.t]\n",
    "        self.t = self.t + 1\n",
    "        dS = self.St() - self.St(self.t - 1)\n",
    "        reward = dS * action\n",
    "        S_t1 = self.observations[self.t]\n",
    "        terminated = True if self.t >= self.tenor else False\n",
    "        can_early_exercise = False\n",
    "        payoff = self.payoff()\n",
    "        return S_t0, S_t1, reward, terminated, can_early_exercise, payoff, dS\n",
    "\n",
    "    def payoff(self, t=None) -> np.float32:\n",
    "        \"\"\"\n",
    "        :return: option payoff if exercise now, regardless it can be exercised, equivalent to moneyless\n",
    "        \"\"\"\n",
    "        return max(0, self.St(t) - self.strike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f6f85bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(n_hidden, N_OBSERVATION):\n",
    "    inputs = layers.Input(shape=(N_OBSERVATION))\n",
    "    x = inputs\n",
    "    for l in n_hidden:\n",
    "        x = layers.Dense(l, activation='relu')(x)\n",
    "    x = layers.Dense(1, activation='tanh')(x)\n",
    "    return tf.keras.Model(inputs, x)\n",
    "\n",
    "\n",
    "def get_critic(n_hidden, N_OBSERVATION):\n",
    "    inputs = layers.Input(shape=(N_OBSERVATION))\n",
    "    x = inputs\n",
    "    for l in n_hidden:\n",
    "        x = layers.Dense(l, activation='relu')(x)\n",
    "    x = layers.Dense(1, activation='sigmoid')(x)\n",
    "    return tf.keras.Model(inputs, x)\n",
    "\n",
    "\n",
    "def get_pretrain_model(n_hidden, days, N_OBSERVATION):\n",
    "    obs_input = layers.Input((days, N_OBSERVATION))\n",
    "    dS_input = layers.Input((days, 1))\n",
    "    x = obs_input\n",
    "    for l in n_hidden:\n",
    "        x = layers.Dense(l, activation='relu')(x)\n",
    "    x = layers.Dense(1, activation='tanh')(x)\n",
    "    pretrain_actor = tf.keras.Model(obs_input, x)\n",
    "    sum_hedge_pl = tf.reduce_sum(dS_input * x, axis=(1, 2))\n",
    "    pretrainer = tf.keras.Model((obs_input, dS_input), sum_hedge_pl)\n",
    "    return pretrain_actor, pretrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebcfc957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer():\n",
    "    \"\"\"\n",
    "    S_t0, S_t1, reward, terminated, can_early_exercise, payoff, dS\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, N_OBSERVATION):\n",
    "        self.size = size\n",
    "\n",
    "        def blank_array(dim):\n",
    "            assert dim <= 2\n",
    "            dim = N_OBSERVATION if dim == 2 else 1\n",
    "            return np.zeros((size, dim), dtype=np.float32)\n",
    "\n",
    "        self.storage = [blank_array(2), blank_array(2),\n",
    "                        blank_array(1), blank_array(1), blank_array(1), blank_array(1), blank_array(1)]\n",
    "        # order: S_t0, S_t1, reward, terminated, can_early_exercise, payoff, dS\n",
    "        self.count = 0\n",
    "\n",
    "    def store(self, values):\n",
    "        index = self.count % self.size\n",
    "        for storage, value in zip(self.storage, values):\n",
    "            storage[index, :] = value\n",
    "        self.count = self.count + 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indexes = np.random.choice(self.size, batch_size, False)\n",
    "        return [v[indexes] for v in self.storage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f633393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodeBuffer():\n",
    "    def __init__(self, capacity, ep_length, N_OBSERVATION):\n",
    "        self.capacity = capacity\n",
    "        self.ep_count = 0\n",
    "\n",
    "        def blank_array(dim):\n",
    "            assert dim <= 2\n",
    "            dim = N_OBSERVATION if dim == 2 else 1\n",
    "            return np.zeros((capacity, ep_length, dim), dtype=np.float32)\n",
    "\n",
    "        self.storage = [blank_array(2), blank_array(2),\n",
    "                        blank_array(1), blank_array(1), blank_array(1), blank_array(1), blank_array(1)]\n",
    "        # order: S_t0, S_t1, reward, terminated, can_early_exercise, payoff, dS\n",
    "\n",
    "    def store(self, values, t):\n",
    "        ep_index = self.ep_count % self.capacity\n",
    "        for storage, value in zip(self.storage, values):\n",
    "            storage[ep_index, t, :] = value\n",
    "        done = values[3]\n",
    "        if done:\n",
    "            self.ep_count = self.ep_count + 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indexes = np.random.choice(self.capacity, batch_size, False)\n",
    "        return [v[indexes] for v in self.storage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d5f094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_episode_wise(env, buffer: EpisodeBuffer, episodes, action=0.5):\n",
    "    for i in range(episodes):\n",
    "        env.reset()\n",
    "        while True:\n",
    "            data = env.step(action)  # actual delta still doesn't matter, avoid calling actor to save time\n",
    "            done = data[3]\n",
    "            buffer.store(data, env.t - 1)\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6eb0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrain_actor(env, n_hidden, n_samples, epoch=12):\n",
    "    N_OBSERVATION = env.n_observation\n",
    "    days = env.tenor\n",
    "    buffer = EpisodeBuffer(n_samples, days, VanillaEnv.n_observation)\n",
    "    gather_episode_wise(env, buffer, n_samples)\n",
    "    observations, dS, payoff = buffer.storage[0], buffer.storage[-1], buffer.storage[-2][:, -1, 0]\n",
    "    pretrain_actor, pretrainer = get_pretrain_model(n_hidden, days, N_OBSERVATION)  #\n",
    "    pretrainer.compile(loss=tf.keras.losses.mse, optimizer=\"Adam\")\n",
    "    pretrainer.fit((observations, dS), payoff, 64, epoch)\n",
    "    actor = get_actor(n_hidden, N_OBSERVATION)\n",
    "    actor.set_weights(pretrain_actor.get_weights())\n",
    "    return actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aec83364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data_pretrain_critic(pretrain_actor, env, n_samples):\n",
    "    df = env.df()\n",
    "    days = env.tenor\n",
    "    buffer = EpisodeBuffer(n_samples, days, VanillaEnv.n_observation)\n",
    "    gather_episode_wise(env, buffer, n_samples)\n",
    "    S_t0, S_t1, reward, terminated, can_early_exercise, payoff, dS = buffer.storage\n",
    "    y_t1 = payoff[:, -1, :]\n",
    "    Y_hedge = np.zeros_like(payoff, dtype=np.float32)\n",
    "    for i in reversed(range(days)):\n",
    "        def reshape(arr):\n",
    "            return arr[:, i, :]\n",
    "\n",
    "        hedge_pl = reshape(dS) * pretrain_actor(reshape(S_t0)).numpy()\n",
    "        y_t0 = y_t1 * df - hedge_pl\n",
    "        y_t0 = np.maximum(0, y_t0)\n",
    "        Y_hedge[:, i, :] = y_t0\n",
    "        y_t1 = y_t0\n",
    "    return Y_hedge, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e36622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrain_critic(env, actor, n_hidden, N_OBSERVATION, n_samples, epoc=10):\n",
    "    Y_hedge, buffer = gather_data_pretrain_critic(actor, env, n_samples)\n",
    "    S_t0, S_t1, reward, terminated, can_early_exercise, payoff, dS = buffer.storage\n",
    "\n",
    "    def reshape(arr):\n",
    "        return arr.reshape((-1, arr.shape[-1]))\n",
    "\n",
    "    critic = get_critic(n_hidden, N_OBSERVATION)\n",
    "    critic.compile(loss=tf.keras.losses.mse, optimizer=\"Adam\")\n",
    "    critic.fit(reshape(S_t0), reshape(Y_hedge), 64, epoc)\n",
    "    return critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a68b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_valuation(terminated, model_valuation, payoff):\n",
    "    return terminated * payoff + (1 - terminated) * model_valuation\n",
    "\n",
    "\n",
    "def get_critic_loss(model, target_model, S_t0, S_t1, reward, terminated, payoff, df):\n",
    "    learn_target = next_valuation(terminated, target_model(S_t1), payoff) * df - reward\n",
    "    return tf.math.reduce_mean(tf.math.square(learn_target - model(S_t0)))\n",
    "\n",
    "\n",
    "def get_actor_loss(actor, critic, S_t0, S_t1, dS, df, mu):\n",
    "    delta = actor(S_t0)\n",
    "    hedge_pl = delta * (dS - mu)\n",
    "    learn_target = critic(S_t1) * df - critic(S_t0)\n",
    "    return tf.math.reduce_mean(tf.math.square(learn_target - hedge_pl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d20cf144",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def learn(train_data, actor, critic, critic_target, optimizer_critic, optimizer_actor, df, mu):\n",
    "    S_t0, S_t1, reward, terminated, can_early_exercise, payoff, dS = train_data\n",
    "    with tf.GradientTape() as tape:\n",
    "        # todo: dcf shold apply interest rate\n",
    "        critic_loss = get_critic_loss(critic, critic_target, S_t0, S_t1, reward, terminated, payoff, df)\n",
    "        critic_gradient = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "        optimizer_critic.apply_gradients(zip(critic_gradient, critic.trainable_variables))\n",
    "    with tf.GradientTape() as tape:\n",
    "        actor_loss = get_actor_loss(actor, critic, S_t0, S_t1, dS, df, mu)\n",
    "        actor_gradient = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "        optimizer_actor.apply_gradients(zip(actor_gradient, actor.trainable_variables))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def soft_update(target_weights, weights, tau):\n",
    "    for (old_value, new_value) in zip(target_weights, weights):\n",
    "        old_value.assign(new_value * tau + old_value * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edfbf7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain actor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 16:48:34.240635: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-04 16:48:34.241034: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4096 samples\n",
      "Epoch 1/12\n",
      "4096/4096 [==============================] - 6s 1ms/sample - loss: 3.1181e-04\n",
      "Epoch 2/12\n",
      "4096/4096 [==============================] - 1s 181us/sample - loss: 2.6345e-04\n",
      "Epoch 3/12\n",
      "4096/4096 [==============================] - 1s 160us/sample - loss: 2.1425e-04\n",
      "Epoch 4/12\n",
      "4096/4096 [==============================] - 1s 161us/sample - loss: 1.8126e-04\n",
      "Epoch 5/12\n",
      "4096/4096 [==============================] - 1s 135us/sample - loss: 1.5425e-04\n",
      "Epoch 6/12\n",
      "4096/4096 [==============================] - 0s 119us/sample - loss: 1.3510e-04\n",
      "Epoch 7/12\n",
      "4096/4096 [==============================] - 0s 122us/sample - loss: 1.1656e-04\n",
      "Epoch 8/12\n",
      "4096/4096 [==============================] - 0s 117us/sample - loss: 1.1293e-04\n",
      "Epoch 9/12\n",
      "4096/4096 [==============================] - 0s 119us/sample - loss: 1.0536e-04\n",
      "Epoch 10/12\n",
      "4096/4096 [==============================] - 1s 128us/sample - loss: 9.8221e-05\n",
      "Epoch 11/12\n",
      "4096/4096 [==============================] - 0s 120us/sample - loss: 9.1963e-05\n",
      "Epoch 12/12\n",
      "4096/4096 [==============================] - 0s 121us/sample - loss: 8.8715e-05\n",
      "pretrain critic\n",
      "Train on 122880 samples\n",
      "Epoch 1/5\n",
      "122880/122880 [==============================] - 8s 69us/sample - loss: 0.0028\n",
      "Epoch 2/5\n",
      "122880/122880 [==============================] - 6s 50us/sample - loss: 1.1642e-04\n",
      "Epoch 3/5\n",
      "122880/122880 [==============================] - 6s 52us/sample - loss: 9.7710e-05\n",
      "Epoch 4/5\n",
      "122880/122880 [==============================] - 6s 52us/sample - loss: 9.0030e-05\n",
      "Epoch 5/5\n",
      "122880/122880 [==============================] - 7s 58us/sample - loss: 8.9868e-05\n"
     ]
    }
   ],
   "source": [
    "S0, r, vol, days, strike = 1, 0.01, 0.3, 30, 1.1\n",
    "n_samples = 2 ** 12\n",
    "n_hidden = [64, 64]\n",
    "process = BlackProcess(S0, r, vol, days)\n",
    "N_OBSERVATION = VanillaEnv.n_observation\n",
    "env = VanillaEnv(process, days, strike)\n",
    "print(\"pretrain actor\")\n",
    "actor = get_pretrain_actor(env, n_hidden, n_samples)\n",
    "env = VanillaEnv(process, days, S0)\n",
    "print(\"pretrain critic\")\n",
    "critic, _ = get_pretrain_critic(env, actor, n_hidden, N_OBSERVATION, n_samples, epoc=5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e43f92d-fa13-40d0-9b7f-72fad67f812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train like actor-critic\n",
      "episode 0\n",
      "WARNING:tensorflow:Layer dense_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "total hedge P/L: 0.0054, option payoff: 0.0000\n",
      "episode 1\n",
      "total hedge P/L: 0.1381, option payoff: 0.1774\n",
      "episode 2\n",
      "total hedge P/L: -0.0078, option payoff: 0.0000\n",
      "episode 3\n",
      "total hedge P/L: 0.0601, option payoff: 0.0438\n",
      "episode 4\n",
      "total hedge P/L: 0.0156, option payoff: 0.0161\n",
      "episode 5\n",
      "total hedge P/L: 0.0057, option payoff: 0.0000\n",
      "episode 6\n",
      "total hedge P/L: 0.0293, option payoff: 0.0379\n",
      "episode 7\n",
      "total hedge P/L: 0.0279, option payoff: 0.0060\n",
      "episode 8\n",
      "total hedge P/L: -0.0182, option payoff: 0.0000\n",
      "episode 9\n",
      "total hedge P/L: 0.0698, option payoff: 0.0589\n",
      "episode 10\n",
      "total hedge P/L: 0.1462, option payoff: 0.1709\n",
      "episode 11\n",
      "total hedge P/L: 0.0207, option payoff: 0.0000\n",
      "episode 12\n",
      "total hedge P/L: 0.0289, option payoff: 0.0106\n",
      "episode 13\n",
      "total hedge P/L: 0.1594, option payoff: 0.1849\n",
      "episode 14\n",
      "total hedge P/L: 0.0649, option payoff: 0.0846\n",
      "episode 15\n",
      "total hedge P/L: -0.0031, option payoff: 0.0000\n",
      "episode 16\n",
      "total hedge P/L: 0.0125, option payoff: 0.0000\n",
      "episode 17\n",
      "total hedge P/L: 0.0149, option payoff: 0.0129\n",
      "episode 18\n",
      "total hedge P/L: 0.0165, option payoff: 0.0000\n",
      "episode 19\n",
      "total hedge P/L: 0.0197, option payoff: 0.0252\n",
      "episode 20\n",
      "total hedge P/L: 0.0300, option payoff: 0.0312\n",
      "episode 21\n",
      "total hedge P/L: 0.0491, option payoff: 0.0589\n",
      "episode 22\n",
      "total hedge P/L: 0.1102, option payoff: 0.1212\n",
      "episode 23\n",
      "total hedge P/L: 0.1656, option payoff: 0.1851\n",
      "episode 24\n",
      "total hedge P/L: -0.0003, option payoff: 0.0000\n",
      "episode 25\n",
      "total hedge P/L: -0.0075, option payoff: 0.0000\n",
      "episode 26\n",
      "total hedge P/L: 0.0395, option payoff: 0.0423\n",
      "episode 27\n",
      "total hedge P/L: 0.0325, option payoff: 0.0454\n",
      "episode 28\n",
      "total hedge P/L: -0.0149, option payoff: 0.0000\n",
      "episode 29\n",
      "total hedge P/L: 0.2052, option payoff: 0.2152\n",
      "episode 30\n",
      "total hedge P/L: 0.0726, option payoff: 0.0709\n",
      "episode 31\n",
      "total hedge P/L: 0.0575, option payoff: 0.0482\n",
      "episode 32\n",
      "total hedge P/L: 0.0867, option payoff: 0.0912\n",
      "episode 33\n",
      "total hedge P/L: 0.0615, option payoff: 0.0622\n",
      "episode 34\n",
      "total hedge P/L: -0.0089, option payoff: 0.0000\n",
      "episode 35\n",
      "total hedge P/L: 0.0594, option payoff: 0.0693\n",
      "episode 36\n",
      "total hedge P/L: 0.0693, option payoff: 0.0810\n",
      "episode 37\n",
      "total hedge P/L: -0.0017, option payoff: 0.0000\n",
      "episode 38\n",
      "total hedge P/L: 0.0581, option payoff: 0.0594\n",
      "episode 39\n",
      "total hedge P/L: -0.0041, option payoff: 0.0000\n"
     ]
    }
   ],
   "source": [
    "critic_target = get_critic(n_hidden, N_OBSERVATION)\n",
    "critic_target.set_weights(critic.get_weights())\n",
    "optimizer_critic = tf.keras.optimizers.Adam(0.002)\n",
    "optimizer_actor = tf.keras.optimizers.Adam(0.002)\n",
    "learn_per_step = 1\n",
    "buffer = Buffer(1024, N_OBSERVATION)\n",
    "batch_size = 32\n",
    "tau = 0.1\n",
    "df = env.df()\n",
    "mu = env.mu()\n",
    "print(\"train like actor-critic\")\n",
    "for eps in range(40):\n",
    "    print(\"episode {}\".format(eps))\n",
    "    epsode_reward = 0\n",
    "    S_t0 = env.reset()[np.newaxis, :]\n",
    "    epsode_reward += critic(S_t0)\n",
    "    while True:\n",
    "        action = actor(S_t0)\n",
    "        step_result = env.step(action)\n",
    "        _, S_t1, reward, terminated, can_early_exercise, payoff, dS = step_result\n",
    "        epsode_reward += reward\n",
    "\n",
    "        buffer.store(step_result)\n",
    "        if buffer.count > batch_size:\n",
    "            train_data = buffer.sample(batch_size)\n",
    "            for _ in range(learn_per_step):\n",
    "                learn(train_data, actor, critic, critic_target, optimizer_critic, optimizer_actor, df, mu)\n",
    "            soft_update(critic_target.variables, critic.variables, tau)\n",
    "        if terminated:\n",
    "            print(\"total hedge P/L: {:5.4f}, option payoff: {:5.4f}\".format(epsode_reward[0, 0], payoff))\n",
    "            break\n",
    "        S_t0 = S_t1[np.newaxis, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02f3aa42-ff76-4d41-9a7d-f22180f3f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.scimath import log, sqrt\n",
    "from scipy import stats\n",
    "\n",
    "cdf = stats.norm.cdf\n",
    "pdf = stats.norm.pdf\n",
    "\n",
    "def d1(s, k, t, r, q, v):\n",
    "    return (log(s / k) + (r - q + 0.5 * v ** 2) * t) / (v * sqrt(t))\n",
    "\n",
    "def d2(s, k, t, r, q, v):\n",
    "    return d1(s, k, t, r, q, v) - v * sqrt(t)\n",
    "\n",
    "def call(s, k, t, r, q, v):\n",
    "    d1_ = d1(s, k, t, r, q, v)\n",
    "    d2_ = d2(s, k, t, r, q, v)\n",
    "    return s * exp(-q * t) * cdf(d1_) - k * exp(-r * t) * cdf(d2_)\n",
    "\n",
    "def delta(s, k, t, r, q, v, isCall):\n",
    "    isPut = np.bitwise_not(isCall)\n",
    "    d1_ = d1(s, k, t, r, q, v)\n",
    "    return exp(-q * t) * cdf(d1_) * isCall + -exp(-q * t) * cdf(-d1_) * isPut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "781331aa-b975-41ec-81cd-f92fec79ab33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by RL model: \n",
      " option value: 0.0352, delta: 0.5162\n",
      "by black-scholes model: \n",
      " option value: 0.0347, delta: 0.5210\n"
     ]
    }
   ],
   "source": [
    "S_t0 = env.reset()[np.newaxis, :]\n",
    "print(\"by RL model: \\n option value: {:5.4f}, delta: {:5.4f}\".format(critic(S_t0)[0, 0], actor(S_t0)[0, 0]))\n",
    "bs_call = call(S0, S0, days / 365, r, 0, vol)\n",
    "bs_delta = delta(S0, S0, days / 365, r, 0, vol, True)\n",
    "print(\"by black-scholes model: \\n option value: {:5.4f}, delta: {:5.4f}\".format(bs_call, bs_delta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
